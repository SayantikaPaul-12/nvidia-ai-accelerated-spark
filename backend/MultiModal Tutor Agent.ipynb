{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86828027-2347-48fb-aed0-e1f70a2b79ca",
   "metadata": {},
   "source": [
    "# üß† AccelLearn: An AI-Powered Multi-Modal Learning Assistant\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) tutor system built with LangGraph and LangChain, accelerated on GPU using Sol's infrastructure.Running this will generate a agentic tutor in gradio .We can run either local LLM on ASU SOL Supercomputer using Ollama Module or call LLM with Api key for MyAPIBuilder(need to uncomment code under section 8)\n",
    "\n",
    "It will also create embedded vector database(Chroma Database) locally in the present directory with Huggingface emdedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f6e28-f83b-4faa-942e-5e4a77c98f55",
   "metadata": {},
   "source": [
    "## 1. Environment Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c8a4c8ec-13ee-48b3-a4e6-d29931f4778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables if needed\n",
    "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/109.0\"\n",
    "\n",
    "ffmpeg_bin = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/\\\n",
    "gcc-12.1.0/ffmpeg-6.0-vsz5thzaks4n56lozbr5sfiwt2djrrga/bin\"\n",
    "os.environ[\"PATH\"] = ffmpeg_bin + os.pathsep + os.environ.get(\"PATH\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ed1cf-5f11-45a8-87ab-3463cfa67d4b",
   "metadata": {},
   "source": [
    "## 2. üì¶ Install Required Libraries\n",
    "Install missing packages for LangChain, LangGraph, SentenceTransformers, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e7d967e-30d5-4baa-86d7-bcfc71051e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.tools import Tool\n",
    "from langgraph.graph import Graph\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import subprocess\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7186f1-3f8d-4ccc-9f18-5cdf14979311",
   "metadata": {},
   "source": [
    "## 3. üåê Load Web & GitHub Sources (Optional)\n",
    "\n",
    "This cell fetches **latest GPU-accelerated learning materials** from trusted NVIDIA sources and blogs. It helps keep the AI tutor **current** with tutorials, tools, and research.\n",
    "\n",
    "### üîé What It Does\n",
    "- **Web Loader**: Scrapes key sites (e.g., RAPIDS, CuPy, CUDA, LangChain tutorials).\n",
    "- **GitHub Loader** *(commented)*: Clones repos and loads `.md`, `.py`, `.ipynb`, `.pdf`, etc.\n",
    "\n",
    "Indexed content is added to the vectorstore for semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° When to Comment Out\n",
    "- You‚Äôve already indexed the content\n",
    "- No new updates are needed\n",
    "- To avoid unnecessary scraping during every run\n",
    "\n",
    "> üíæ Tip: Run once, persist to disk, and skip in later runs or production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baeadf22-6214-4dba-b7fe-9193fd943c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tempfile\n",
    "from git import Repo\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    NotebookLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    ")\n",
    "\n",
    "def clone_and_load_all(repo_url: str, branch: str = \"main\"):\n",
    "    \"\"\"\n",
    "    Clone a GitHub repo and load every common document type.\n",
    "    Returns List[Document].\n",
    "    \"\"\"\n",
    "    tmp_dir = tempfile.mkdtemp()\n",
    "    Repo.clone_from(repo_url, tmp_dir, branch=branch)\n",
    "\n",
    "    loaders = [\n",
    "        # plain-text: .md, .py, .txt, .csv, .json, etc.\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.md\", loader_cls=TextLoader),\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.py\", loader_cls=TextLoader),\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.txt\", loader_cls=TextLoader),\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.csv\", loader_cls=TextLoader),\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.json\", loader_cls=TextLoader),\n",
    "        # Jupyter notebooks\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.ipynb\", loader_cls=NotebookLoader),\n",
    "        # PDFs\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
    "        # Word docs\n",
    "        DirectoryLoader(tmp_dir, glob=\"**/*.docx\", loader_cls=UnstructuredWordDocumentLoader),\n",
    "        # ‚Ä¶add more loaders here if you need Excel, PowerPoint, HTML, etc.\n",
    "    ]\n",
    "\n",
    "    doc = []\n",
    "    for loader in loaders:\n",
    "        doc.extend(loader.load())\n",
    "    return doc\n",
    "\n",
    "# ‚Äî your inputs ‚Äî\n",
    "web_urls = [\n",
    "    # Core NVIDIA Tools\n",
    "    \"https://rapids.ai\",\n",
    "    \"https://cupy.dev/\",\n",
    "    \"https://catalog.ngc.nvidia.com/\",\n",
    "    # Learning Platforms\n",
    "    \"https://www.nvidia.com/en-us/training/\",\n",
    "    # Supporting Tools\n",
    "    \"https://developer.nvidia.com/cuda-toolkit\",\n",
    "    \"https://developer.nvidia.com/nsight-systems\",\n",
    "    # NVIDIA GPU Acceleration Materials\n",
    "    \"https://github.com/NVIDIA/accelerated-computing-hub/tree/main\",\n",
    "    \"https://python.langchain.com/docs/integrations/document_loaders/source_code/\",\n",
    "    \"https://developer.nvidia.com/blog/category/data-science/\",\n",
    "    \"https://developer.nvidia.com/blog/rapids-brings-zero-code-change-acceleration-io-performance-gains-and-out-of-core-xgboost/\",\n",
    "    \"https://developer.nvidia.com/blog/ai-in-manufacturing-and-operations-at-nvidia-accelerating-ml-models-with-nvidia-cuda-x-data-science/\",\n",
    "    # RAPIDS blog\n",
    "    \"https://medium.com/rapids-ai/rapids-23-08-release-23db51c255f0\",\n",
    "    \"https://medium.com/rapids-ai/easy-cpu-gpu-arrays-and-dataframes-run-your-dask-code-where-youd-like-e349d92351d\",\n",
    "    # CuPy blogs\n",
    "    \"https://medium.com/cupy-team/announcing-cupy-v13-66979ee7fab0\",\n",
    "    \"https://www.unum.cloud/blog/2022-01-26-cupy\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/\",\n",
    "]\n",
    "#git_repos = [\n",
    "#    \"https://github.com/NVIDIA/accelerated-computing-hub.git\",\n",
    "#]\n",
    "\n",
    "# ‚Äî load everything ‚Äî\n",
    "docs = []\n",
    "\n",
    "# 1) load web pages\n",
    "for url in web_urls:\n",
    "    docs.extend(WebBaseLoader(url).load())\n",
    "\n",
    "2) clone & load each GitHub repo\n",
    "for repo in git_repos:\n",
    "   docs.extend(clone_and_load_all(repo))\n",
    "\n",
    "# now `all_docs` contains Document objects for:\n",
    "#   ‚Ä¢ every web page you listed\n",
    "#   ‚Ä¢ every README, .py, .ipynb, .txt, .pdf, .docx, etc. in each repo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd29e1b-18da-4196-9d6e-1ce476d60bc2",
   "metadata": {},
   "source": [
    "## 4. üîç Preview the first 1000 characters of the first document loaded.\n",
    " This is useful to inspect whether the WebBaseLoader or repo loader has captured clean and relevant content.\n",
    " Helpful for debugging or verifying that scraping/parsing was successful.\n",
    " ‚ö†Ô∏è If docs is empty (no documents loaded), this will raise an IndexError.\n",
    " ‚úÖ Best used right after loading web or repo content to visually confirm correctness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "698a9629-f9bb-4c24-8fbd-76ea13e5e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs[0].page_content.strip()[:1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da654b40-e8d2-4894-a29e-46e8c76c4b18",
   "metadata": {},
   "source": [
    "### 5. Split the fetched documents into smaller chunks for indexing into the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2055b15-d42c-42c6-aff7-969b539787c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "doc_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7224e-99e2-4e53-a7df-59c18ce59c66",
   "metadata": {},
   "source": [
    "## 6. üß† Vector Store Setup for GPU Acceleration Documents\n",
    "\n",
    "This section initializes a **persistent Chroma vector database** using **HuggingFace embeddings**. It allows us to efficiently store, index, and retrieve documents related to GPU acceleration for use in Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step-by-Step Breakdown:\n",
    "\n",
    "1. **Import Libraries**\n",
    "   - `langchain_chroma`: Provides the Chroma vector database integration.\n",
    "   - `langchain_huggingface`: Allows embedding documents using HuggingFace models.\n",
    "\n",
    "2. **Initialize Embedding Model**\n",
    "   ```python\n",
    "   embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "3. **Vector Store Initialization with Chroma**\n",
    "\n",
    "    We use **Chroma** as our Persistent vector database to store and retrieve GPU-acceleration documents efficiently. This is essential for enabling **semantic search** in Retrieval-Augmented Generation (RAG) workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "58ae5c00-007c-4212-a361-d0eec2beabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=\"./Gpu_acceleration_news_latest\"\n",
    ")\n",
    "#uncomment the below line when new documents need to be added after fetching documents\n",
    "\n",
    "vectorstore.add_documents(doc_splits)   \n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d8f10-ac02-43fe-99ea-57ea4c810bfd",
   "metadata": {},
   "source": [
    "### 7. Create a retriever tool using LangChain's prebuild `create_retriever_tool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7394fbfc-68d3-45e3-9d2c-83ca8549692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_documents\",\n",
    "    \"Search all uploaded documents and web content for information relevant to the user's question. Use this for resumes, web knowledge, technical docs, and any stored content.\"\n",
    ")\n",
    "#uncomment below to Test the tool(Optional)\n",
    "\n",
    "#retriever_tool.invoke({\"query\": \"tell me about nsight systems\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a631e52-2278-48dc-9ee1-c3cb7a05c757",
   "metadata": {},
   "source": [
    "## 8. Load LLM\n",
    "###  Load local LLM\n",
    "\n",
    "Start ollama using the terminal:\n",
    "```bash\n",
    "module load ollama/0.9.0\n",
    "export OLLAMA_MODELS=/data/datasets/community/ollama\n",
    "ollama-start\n",
    "```\n",
    "\n",
    "### Load LLM using Api Key from MyAiBuilder \n",
    "To load LLM using Api key,uncomment the below section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2933c977-ca9d-47fe-9574-9a075159393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models.base import BaseChatModel\n",
    "# from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "# from langchain_core.outputs import ChatResult, ChatGeneration\n",
    "# from typing import List\n",
    "# import requests\n",
    "\n",
    "# def init_chat_model(*args, **kwargs):\n",
    "#     class ASUChatLLM(BaseChatModel):\n",
    "#         api_url: str = \"https://api-main-beta.aiml.asu.edu/query\"\n",
    "#         api_key: str = \"\"  # add with your key\n",
    "\n",
    "#         def _call_api(self, prompt: str) -> str:\n",
    "#             headers = {\n",
    "#                 \"Content-Type\": \"application/json\",\n",
    "#                 \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "#             }\n",
    "#             payload = {\"query\": prompt}\n",
    "#             response = requests.post(self.api_url, json=payload, headers=headers)\n",
    "#             response.raise_for_status()\n",
    "#             return response.json()[\"response\"]\n",
    "\n",
    "#         def _format_messages(self, messages: List) -> str:\n",
    "#             parts = []\n",
    "#             for m in messages:\n",
    "#                 if isinstance(m, HumanMessage):\n",
    "#                     parts.append(f\"User: {m.content}\")\n",
    "#                 elif isinstance(m, AIMessage):\n",
    "#                     parts.append(f\"AI: {m.content}\")\n",
    "#                 elif isinstance(m, SystemMessage):\n",
    "#                     parts.append(f\"System: {m.content}\")\n",
    "#             return \"\\n\".join(parts)\n",
    "\n",
    "#         def _generate(self, messages: List, stop: List[str] = None) -> ChatResult:\n",
    "#             prompt = self._format_messages(messages)\n",
    "#             output = self._call_api(prompt)\n",
    "#             return ChatResult(generations=[ChatGeneration(message=AIMessage(content=output))])\n",
    "\n",
    "#         @property\n",
    "#         def _llm_type(self) -> str:\n",
    "#             return \"asu_chat_llm\"\n",
    "\n",
    "#         def bind_tools(self, tools, tool_choice=None, **kwargs):\n",
    "#     # ASU API doesn't support tool calls, so we no-op this\n",
    "#             return self\n",
    "\n",
    "\n",
    "#     return ASUChatLLM()\n",
    "\n",
    "# # override the original call\n",
    "# host_node = \"ignored\"\n",
    "# llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0, base_url=f\"http://{host_node}:11434/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "223ac50b-8a6c-4a41-a270-f4e4dc23f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "import socket\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "host_node = socket.gethostname()\n",
    "llm_model = init_chat_model(\"ollama:qwen3:14b\", temperature=0, base_url=f\"http://jgarc111@{host_node}:11434/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17110369-f393-43ce-9d9f-0dbd28248474",
   "metadata": {},
   "source": [
    "### 9. Build a `generate_query_or_respond` node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ce01200b-a182-4e8a-92bc-0478389b5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "import time, re\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    print(\"‚è≥ Calling LLM with tools...\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    response = (\n",
    "        llm_model\n",
    "        .bind_tools([retriever_tool])\n",
    "        .invoke(state[\"messages\"])\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ LLM responded in\", round(time.time() - t0, 2), \"seconds\")\n",
    "\n",
    "    # Clean hallucinated tags\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "\n",
    "    if \"tool_calls\" in response.additional_kwargs:\n",
    "        return {\"tool\": retriever_tool.name}\n",
    "    else:\n",
    "        return {\"messages\": [response]}\n",
    "##uncomment below to try a random input\n",
    "\n",
    "# response = llm_model.invoke([\n",
    "#     {\"role\": \"user\", \"content\": \"Hello! what is the color of a rainbow?\"}\n",
    "# ])\n",
    "# print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726340c6-2668-4956-ad9f-327284624c3a",
   "metadata": {},
   "source": [
    "## 10.  Grade documents\n",
    "###  Add conditional edge `grade_documents` to determine the relevance of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c472028e-398d-4d38-80f7-556c5e270daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    output = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "# Extract score manually from string\n",
    "    text = output.content.strip().lower()\n",
    "\n",
    "# Acceptable variants like \"yes.\", \"no!\", etc.\n",
    "    if \"yes\" in text:\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc175468-3493-458f-bb16-1e4cae54961b",
   "metadata": {},
   "source": [
    "## 11. ‚úçÔ∏è Question Rewriting for Semantic Clarity\n",
    "\n",
    "This function takes a user's original question and rewrites it to improve its **semantic clarity** and **retrievability**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üîß How It Works\n",
    "\n",
    "1. **Prompt Template**\n",
    "   - The `REWRITE_PROMPT` instructs the LLM to analyze and improve the input question.\n",
    "   - It explicitly asks the model to preserve the original meaning but improve its form.\n",
    "\n",
    "2. **`rewrite_question()` Function**\n",
    "   - Extracts the first user message.\n",
    "   - Injects it into the prompt and calls the LLM (e.g., via `invoke()`).\n",
    "   - Strips away any special `<think>...</think>` tokens sometimes returned by certain models.\n",
    "   - Returns a new message object with the rewritten question.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "057d8d16-008b-4938-bf7c-9dda7cc5a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee26b6-a172-463c-ada1-8558a9305f46",
   "metadata": {},
   "source": [
    "## 12. Build `generate_answer` node to answer from Retrieved Context\n",
    "\n",
    "This section defines how the AI tutor generates concise and relevant answers using an LLM and a set of retrieved documents.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Purpose\n",
    "\n",
    "The `generate_answer()` function is the core of the Retrieval-Augmented Generation (RAG) process. It takes the user‚Äôs question and a set of retrieved context documents, then formulates a **brief, accurate response**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3db60a85-0fc5-4328-8b69-9e22d62119e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    # remove thinking text\n",
    "    content = re.sub(r\"<think>.*</think>\", \"\", response.content, flags=re.DOTALL).strip()\n",
    "    response.content = content\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5867ccc-463c-49e6-8180-6eb4a53a2325",
   "metadata": {},
   "source": [
    "# 13. Assemble the graph\n",
    "## üß© LangGraph Workflow: RAG-Based Question Answering\n",
    "\n",
    "This section defines a **LangGraph-based state machine** that models the logic of a Retrieval-Augmented Generation (RAG) system. It orchestrates how the AI tutor interprets a question, decides to retrieve information, rewrites queries if needed, and finally generates an answer.\n",
    "### üß† Purpose\n",
    "\n",
    "To implement a dynamic, flexible, and **tool-aware graph-based control flow** where:\n",
    "- The user query is assessed and optionally rewritten\n",
    "- External tools like retrievers are conditionally invoked\n",
    "- Answers are generated based on the best-available context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8789dab-ada5-4c34-a1dc-7763224c4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e504d49-8574-4226-9d5f-c1729d3761b7",
   "metadata": {},
   "source": [
    "## 14. üß† Core Tutor Capabilities: Prompts, Uploaders & GPU Tools\n",
    "\n",
    "This cell defines the foundational functions that enable your AI tutor to:\n",
    "- Adapt to different tutoring personas\n",
    "- Ingest documents and videos\n",
    "- Convert CPU code to GPU code\n",
    "- Benchmark performance between CPU and GPU\n",
    "\n",
    "Together, these components make the tutor **flexible, multimodal, and GPU-aware**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 1. ü§ñ Bot Prompts (System Personalities)\n",
    "\n",
    "Defined via `bot_prompts`, these system messages control how the AI behaves. Modes include:\n",
    "\n",
    "- **Socratic Tutor**: Asks guiding questions to promote deep understanding.\n",
    "- **General Purpose Assistant**: Provides friendly, informative answers.\n",
    "- **GPU Benchmarking & Guidance**: Converts and profiles code for GPU acceleration.\n",
    "- **Quiz Mode**: Runs timed MCQs with scoring and feedback.\n",
    "- **Video Tutorial Bot**: Answers questions using video transcript content.\n",
    "\n",
    "This structure ensures tailored interactions based on the learner‚Äôs goals.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 2. üìÑ Document Uploader\n",
    "\n",
    "The `process_document()` function:\n",
    "- Accepts `.pdf` or `.txt` files\n",
    "- Splits them into 500-character chunks\n",
    "- Embeds them with HuggingFace model\n",
    "- Adds them to a persistent vectorstore (Chroma)\n",
    "\n",
    "These chunks are searchable using semantic similarity, powering Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 3. üé• Video Uploader\n",
    "\n",
    "The `process_video()` function:\n",
    "- Converts uploaded `.mp4` to `.wav` using FFMPEG\n",
    "- Transcribes audio to English using Whisper (HuggingFace)\n",
    "- Splits the transcript into searchable chunks\n",
    "- Adds them to the same vectorstore\n",
    "\n",
    "Video transcripts become part of the knowledge base, enabling Q&A over lecture recordings.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 4. ‚öôÔ∏è GPU Code Generation\n",
    "\n",
    "When users submit Python code:\n",
    "- The system detects it with regex\n",
    "- Sends the CPU code to an LLM with specific GPU conversion instructions (NumPy ‚Üí CuPy, etc.)\n",
    "- Extracts and returns the GPU-compatible version\n",
    "\n",
    "This enables quick prototyping and learning of GPU-accelerated techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ 5. ‚è±Ô∏è Benchmarking Engine\n",
    "\n",
    "The `benchmark_cpu_vs_gpu()` function:\n",
    "- Runs both CPU and GPU versions of code in parallel threads\n",
    "- Times each run, measures memory, and samples GPU metrics (utilization, power)\n",
    "- Computes speedup and generates performance summaries\n",
    "\n",
    "This is ideal for helping learners **see the real-world impact** of GPU acceleration.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ How It All Works Together\n",
    "\n",
    "1. The user selects a bot persona.\n",
    "2. They can upload docs or videos to enrich the tutor‚Äôs knowledge.\n",
    "3. They ask questions or submit code.\n",
    "4. The system:\n",
    "   - Retrieves context\n",
    "   - Converts and benchmarks code\n",
    "   - Uses LangGraph to generate a smart, context-aware reply\n",
    "5. Results are displayed in a chat interface ‚Äî fast, helpful, and GPU-literate.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° This all-in-one engine turns your notebook into a **modular, intelligent GPU tutor**, capable of answering questions, running benchmarks, and guiding users through real code transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0d254d71-d15d-4570-bc1c-fdffe5896fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "bot_prompts = {\n",
    "    \"Socratic Tutor\": (\n",
    "        \"You are a Socratic AI tutor. Your goal is to help learners deeply understand \"\n",
    "        \"concepts related to data science and GPU acceleration. You do **not give direct answers**. \"\n",
    "        \"Instead, you ask thoughtful, guiding questions to promote critical thinking and discovery. \"\n",
    "        \"Use clear, concise language and only escalate to hints if the student is stuck. Be curious, never judgmental.\"\n",
    "    ),\n",
    "    \"General Purpose Assistant\": (\n",
    "        \"You are a helpful AI assistant focused on data science and GPU acceleration. \"\n",
    "        \"Provide concise, clear, and complete answers. Your tone should be friendly and professional. \"\n",
    "        \"If code is required, write it cleanly and explain it. When appropriate, explain trade-offs or next steps.\"\n",
    "    ),\n",
    "    \"GPU Benchmarking & Guidance\": (\n",
    "        \"You are an expert AI tutor in GPU acceleration for data science workflows. \"\n",
    "        \"Your job is to help users convert CPU-based code (e.g., NumPy, Pandas, Scikit-learn) \"\n",
    "        \"into GPU-accelerated versions using CuPy, cuDF, cuML, or Numba. After conversion, \"\n",
    "        \"suggest how to benchmark and compare performance. Provide code or ideas or examples. \"\n",
    "        \"You may include performance profiling tips, explain what the code is doing step by step, \"\n",
    "        \"and how to evaluate gains.\"\n",
    "    ),\n",
    "    \"üìù Quiz Mode\": (\n",
    "        \"You are the AI Tutor‚Äôs Quiz Mode engine. Your job is to conduct structured multiple-choice quizzes \"\n",
    "        \"and deliver detailed feedback after completion. Follow this exact behavior:\\n\\n\"\n",
    "        \"**Quiz Rules:**\\n\"\n",
    "        \"- When the user requests a quiz, confirm the topic.\\n\"\n",
    "        \"- Start a 10-question multiple-choice quiz (MCQ).\\n\"\n",
    "        \"- Each question must have 4 labeled options (A, B, C, D).\\n\"\n",
    "        \"- After each question, wait for user input (A/B/C/D).\\n\"\n",
    "        \"- Do not show answers until the full quiz is complete or 2 minutes have passed.\\n\\n\"\n",
    "        \"**After the Quiz Ends:**\\n\"\n",
    "        \"- Show ‚úÖ Score and total time taken.\\n\"\n",
    "        \"- Provide a full breakdown of each question:\\n\"\n",
    "        \"  ‚Ä¢ The question\\n\"\n",
    "        \"  ‚Ä¢ ‚úÖ Correct answer with explanation\\n\"\n",
    "        \"  ‚Ä¢ ‚ùå Incorrect options with explanations why they‚Äôre wrong\\n\\n\"\n",
    "        \"**Feedback Section:**\\n\"\n",
    "        \"- Identify weak areas based on incorrect answers\\n\"\n",
    "        \"- Suggest 3 clear, actionable upskilling steps using this structure:\\n\"\n",
    "        \"  1. üìñ Read ‚Üí link to docs\\n\"\n",
    "        \"  2. üß™ Practice ‚Üí what to try\\n\"\n",
    "        \"  3. üé• Watch ‚Üí video/tutorial recommendation\\n\\n\"\n",
    "        \"**Tone & Style:**\\n\"\n",
    "        \"Be structured, encouraging, and informative. Use emojis like ‚úÖ ‚ùå ‚ö†Ô∏è üìñ üé• to guide learners visually. \"\n",
    "        \"Keep your language motivational but concise.\\n\\n\"\n",
    "        \"Important: Do NOT reveal answers before the quiz ends. Stay in character as a friendly quiz proctor. \"\n",
    "        \"If the user is mid-quiz, only show the next question.\"\n",
    "    ),\n",
    "    \"üé• Video Tutorial Bot\": (\n",
    "        \"You are a video transcript-based tutor. You assist learners by summarizing, explaining, and answering questions based on uploaded video lectures. Respond clearly and helpfully based on the latest uploaded video content.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Initial message list (will be reset when tutor is selected)\n",
    "messages = []\n",
    "#------------------------------------------------------------------------------\n",
    "#Document Processing Function ===  to embed and add documents to persistent database ===\n",
    "def process_document(file):\n",
    "    if file is None:\n",
    "        return \"Please upload a document.\"\n",
    "\n",
    "    file_path = file.name\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path) if ext == \".pdf\" else TextLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "\n",
    "        vectorstore.add_documents(chunks)\n",
    "\n",
    "        return f\"‚úÖ Uploaded and indexed {len(chunks)} chunks from: {os.path.basename(file_path)}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "#Video Transcript Processor (Whisper + FFMPEG)\n",
    "asr = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    device=0,\n",
    "    # or -1 for CPU,\n",
    "    return_timestamps=True\n",
    ")\n",
    "videosplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "FFMPEG = \"/packages/apps/spack/21/opt/spack/linux-rocky8-zen3/gcc-12.1.0/ffmpeg-6.0-vsz5thzaks4n56lozbr5sfiwt2djrrga/bin/ffmpeg\"\n",
    "def process_video(video_file):\n",
    "    global latest_video_chunks  # enable external access\n",
    "\n",
    "    path = video_file.name\n",
    "    audio_path = path.rsplit(\".\",1)[0] + \".wav\"\n",
    "    subprocess.run([\n",
    "        FFMPEG, \"-y\", \"-i\", path,\n",
    "        \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "        audio_path\n",
    "    ], check=True)\n",
    "\n",
    "    result = asr(audio_path, generate_kwargs={\"task\": \"translate\", \"language\": \"en\"})\n",
    "\n",
    "    transcript = result[\"text\"]\n",
    "\n",
    "    docs = videosplitter.split_text(transcript)\n",
    "    docs = [Document(page_content=chunk) for chunk in docs]\n",
    "\n",
    "    latest_video_chunks = docs  # store for later lookup\n",
    "    vectorstore.add_documents(docs)  # optional: for global search\n",
    "\n",
    "    return f\"‚úÖ Indexed {len(docs)} transcript chunks from video.\"\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# === Set selected tutor and reset history ===\n",
    "def set_bot(bot_name):\n",
    "    global messages\n",
    "    system_prompt = bot_prompts.get(bot_name, \"You are a helpful assistant.\")\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    \n",
    "    # Show video input only for Video Tutorial Bot\n",
    "    show_video = bot_name == \"üé• Video Tutorial Bot\"\n",
    "    return \"\", [], gr.update(visible=show_video)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#Benchmark Plotting: CPU vs GPU Performance\n",
    "def plot_benchmark_chart(cpu_time, gpu_time, save_path=\"/mnt/data/benchmark_chart.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar([\"CPU\", \"GPU\"], [cpu_time, gpu_time], color=[\"skyblue\", \"salmon\"])\n",
    "    ax.set_ylabel(\"Time (seconds)\")\n",
    "    ax.set_title(\"‚è±Ô∏è CPU vs GPU Execution Time\")\n",
    "    ax.text(0, cpu_time + 0.01, f\"{cpu_time:.4f} s\", ha=\"center\")\n",
    "    ax.text(1, gpu_time + 0.01, f\"{gpu_time:.4f} s\", ha=\"center\")\n",
    "    plt.ylim(0, max(cpu_time, gpu_time) * 1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return save_path\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "#`benchmark_cpu_vs_gpu()` ‚Äì End-to-End Code Benchmarking\n",
    "\n",
    "import re, textwrap, time, subprocess, threading, concurrent.futures, os\n",
    "from typing import Optional\n",
    "import psutil\n",
    "\n",
    "# ‚îÄ‚îÄ Regex helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "CODE_FENCE_RE = re.compile(r\"```(?:python)?\\s*(.*?)```\", re.DOTALL | re.IGNORECASE)\n",
    "THINK_RE = re.compile(r\"<think>.*?</think>\", re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "def is_code_snippet(text: str) -> bool:\n",
    "    return bool(CODE_FENCE_RE.search(text) or \"import \" in text)\n",
    "\n",
    "def extract_code(text: str) -> str:\n",
    "    m = CODE_FENCE_RE.search(text)\n",
    "    return textwrap.dedent(m.group(1)) if m else textwrap.dedent(text)\n",
    "\n",
    "# ‚îÄ‚îÄ CPU‚ÜíGPU mapping ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "GPU_LIB_MAP = {\n",
    "    \"numpy\": \"cupy\",\n",
    "    \"pandas\": \"cudf\",\n",
    "    \"sklearn\": \"cuml\",\n",
    "    \"torch\": \"torch.cuda\",\n",
    "}\n",
    "\n",
    "def llm_convert_to_gpu(src: str) -> Optional[str]:\n",
    "    prompt = (\n",
    "        \"You are an expert Python GPU accelerator. \"\n",
    "        \"If the code already uses GPU libraries, reply with the single token NO_CONVERSION_NEEDED. \"\n",
    "        \"Otherwise, rewrite the code to use GPU-accelerated libraries. \"\n",
    "        \"Example mappings:\\n\"\n",
    "        + \"\\n\".join(f\"- {k} ‚Üí {v}\" for k, v in GPU_LIB_MAP.items()) +\n",
    "        \"\\nReturn ONLY valid Python code inside a markdown ```python block. \"\n",
    "        \"If no GPU alternative exists, return the single token NO_GPU_LIB.\\n\\n\"\n",
    "        \"=== CPU Code ===\\n\" + src\n",
    "    )\n",
    "\n",
    "    raw = llm_model.invoke([{\"role\": \"user\", \"content\": prompt}]).content.strip()\n",
    "    raw = THINK_RE.sub(\"\", raw).strip()\n",
    "    if raw.startswith((\"NO_GPU_LIB\", \"NO_CONVERSION_NEEDED\")):\n",
    "        return None\n",
    "\n",
    "    m = CODE_FENCE_RE.search(raw)\n",
    "    return textwrap.dedent(m.group(1) if m else raw).strip()\n",
    "\n",
    "# ‚îÄ‚îÄ GPU metrics sampling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def sample_gpu_metrics(interval=0.1, duration=5.0):\n",
    "    samples = []\n",
    "\n",
    "    def _poll():\n",
    "        start = time.time()\n",
    "        while time.time() - start < duration:\n",
    "            try:\n",
    "                output = subprocess.check_output([\n",
    "                    \"nvidia-smi\",\n",
    "                    \"--query-gpu=utilization.gpu,power.draw\",\n",
    "                    \"--format=csv,noheader,nounits\"\n",
    "                ]).decode().strip()\n",
    "                util, power = map(float, output.split(\",\"))\n",
    "                samples.append((util, power))\n",
    "            except:\n",
    "                samples.append((0.0, 0.0))\n",
    "            time.sleep(interval)\n",
    "\n",
    "    thread = threading.Thread(target=_poll)\n",
    "    thread.start()\n",
    "    return samples, thread\n",
    "\n",
    "# ‚îÄ‚îÄ Timing with metrics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def _timed_exec(code: str, label=\"Code\") -> dict:\n",
    "    import torch\n",
    "\n",
    "    warmup_needed = any(lib in code for lib in (\"cupy\", \"torch.cuda\"))\n",
    "    if warmup_needed:\n",
    "        try:\n",
    "            warmup = (\n",
    "                \"import cupy as cp\\n\"\n",
    "                \"_ = (cp.random.rand(512,512) @ cp.random.rand(512,512)).sum()\\n\"\n",
    "                \"cp.cuda.Device(0).synchronize()\"\n",
    "            ) if \"cupy\" in code else (\n",
    "                \"import torch\\n\"\n",
    "                \"_ = torch.rand(512,512, device='cuda') @ torch.rand(512,512, device='cuda')\\n\"\n",
    "                \"torch.cuda.synchronize()\"\n",
    "            )\n",
    "            exec(warmup, {})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Start GPU monitoring\n",
    "    gpu_metrics, sampler = sample_gpu_metrics(duration=3)\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss\n",
    "\n",
    "    # Optional: reset GPU mem stats\n",
    "    if \"torch\" in code:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    exec(code, {})\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    mem_after = process.memory_info().rss\n",
    "    sampler.join()\n",
    "\n",
    "    cpu_mem = (mem_after - mem_before) / (1024 ** 2)\n",
    "    avg_util = sum(m[0] for m in gpu_metrics) / max(len(gpu_metrics), 1)\n",
    "    avg_power = sum(m[1] for m in gpu_metrics) / max(len(gpu_metrics), 1)\n",
    "    total_energy = sum(p * 0.1 for _, p in gpu_metrics)  # power * interval (0.1s)\n",
    "\n",
    "    gpu_mem = torch.cuda.max_memory_allocated() / (1024 ** 2) if \"torch\" in code else 0\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"time\": round(end - start, 4),\n",
    "        \"cpu_mem_mb\": round(cpu_mem, 2),\n",
    "        \"gpu_mem_mb\": round(gpu_mem, 2),\n",
    "        \"gpu_util\": round(avg_util, 2),\n",
    "        \"gpu_power\": round(avg_power, 2),\n",
    "        \"gpu_energy_j\": round(total_energy, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Benchmark Runner ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def benchmark_cpu_vs_gpu(cpu_code: str, timeout: float = 20.0):\n",
    "    gpu_code = llm_convert_to_gpu(cpu_code)\n",
    "    if gpu_code is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    def _run(label, code):\n",
    "        try:\n",
    "            return _timed_exec(code, label)\n",
    "        except Exception as e:\n",
    "            return {\"label\": label, \"error\": f\"{type(e).__name__}: {e}\"}\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as pool:\n",
    "        fut_cpu = pool.submit(_run, \"CPU\", cpu_code)\n",
    "        fut_gpu = pool.submit(_run, \"GPU\", gpu_code)\n",
    "        cpu_metrics = fut_cpu.result(timeout)\n",
    "        gpu_metrics = fut_gpu.result(timeout)\n",
    "\n",
    "    # Compute speedup\n",
    "    if \"time\" in cpu_metrics and \"time\" in gpu_metrics and gpu_metrics[\"time\"] > 0:\n",
    "        speedup = round(cpu_metrics[\"time\"] / gpu_metrics[\"time\"], 2)\n",
    "    else:\n",
    "        speedup = \"N/A\"\n",
    "\n",
    "    # Full metric dict return (for chart/report)\n",
    "    return {\n",
    "        \"cpu\": cpu_metrics,\n",
    "        \"gpu\": gpu_metrics,\n",
    "        \"gpu_code\": gpu_code,\n",
    "        \"speedup\": speedup\n",
    "    }\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "#Smart Tutoring with Code & Context Handling\n",
    "\n",
    "\n",
    "latest_video_chunks = []  # stores last transcript\n",
    "\n",
    "# === Chat handling function ===\n",
    "def ask_graph(user_input, chat_history):\n",
    "    global messages, latest_video_chunks, llm\n",
    "\n",
    "    # Handle code snippets: benchmark CPU vs GPU, then have LLM craft a friendly report including GPU code\n",
    "    if is_code_snippet(user_input):\n",
    "        cpu_src = extract_code(user_input)\n",
    "        out = benchmark_cpu_vs_gpu(cpu_src)\n",
    "\n",
    "        if not out or out.get(\"gpu_code\") is None:\n",
    "            assistant_reply = (\n",
    "                \"‚ÑπÔ∏è I couldn‚Äôt find a GPU-accelerated replacement for the \"\n",
    "                \"libraries used in your code, so no benchmark was run.\"\n",
    "            )\n",
    "        else:\n",
    "            cpu_time       = out[\"cpu\"][\"time\"]\n",
    "            gpu_time       = out[\"gpu\"][\"time\"]\n",
    "            speedup_factor = out[\"speedup\"]\n",
    "            gpu_src        = out[\"gpu_code\"]\n",
    "            cpu_mem = out[\"cpu\"].get(\"cpu_mem_mb\", 0)\n",
    "            gpu_mem = out[\"gpu\"].get(\"gpu_mem_mb\", 0)\n",
    "            gpu_util = out[\"gpu\"].get(\"gpu_util\", 0)\n",
    "            gpu_power = out[\"gpu\"].get(\"gpu_power\", 0)\n",
    "            gpu_energy = out[\"gpu\"].get(\"gpu_energy_j\", 0)\n",
    "\n",
    "\n",
    "            # Prompt the LLM to include the GPU code and explain performance\n",
    "            # summary_prompt = (\n",
    "            #     f\"I ran your original code on CPU and measured a runtime of {cpu_time:.4f} seconds. \"\n",
    "            #     f\"I also converted it to GPU-accelerated code as shown below:\\n\\n\"\n",
    "            #     f\"```python\\n{gpu_src}\\n```\\n\\n\"\n",
    "            #     f\"The GPU version ran in {gpu_time:.4f} seconds, achieving a {speedup_factor:.2f}√ó speedup. \"\n",
    "            #     \"Please generate a concise, user-friendly report that includes the GPU code snippet, \"\n",
    "            #     \"describes what changes were made for GPU acceleration, and explains the performance improvement.\"\n",
    "            # )\n",
    "            summary_prompt = f\"\"\"\n",
    "You are a performance analysis assistant. Compare the following benchmark results for a CPU and GPU version of Python code.\n",
    "only compare known facts which present GPU better than CPU.And also compulsorily give the GPU version code{gpu_src} at the top in readable format.\n",
    "\n",
    "Use this table:\n",
    "\n",
    "| Metric        | CPU           | GPU           | Units      |\n",
    "|---------------|---------------|----------------|------------|\n",
    "| Time          | {cpu_time:.4f} | {gpu_time:.4f} | sec        |\n",
    "| RAM Used      | {cpu_mem:.2f} | {gpu_mem:.2f}  | MB         |\n",
    "\n",
    "\n",
    "Explain:\n",
    "1. how much time it took to run in cpu?\n",
    "2.how much time it took to run on GPU?\n",
    "3. What trade-offs might a beginner want to consider?\n",
    "Be clear and concise and positive about GPU acceleration. Use bullet points and readable formatting.\n",
    "\"\"\"\n",
    "\n",
    "            result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": summary_prompt}]})\n",
    "            assistant_reply = result[\"messages\"][-1].content\n",
    "        # Update message history\n",
    "        messages.append({\"role\": \"user\",    \"content\": user_input})\n",
    "        messages.append({\"role\": \"assistant\",\"content\": assistant_reply})\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "        return \"\", chat_history\n",
    "\n",
    "        \n",
    "    # ---------- VIDEO CONTEXT PATH ---------------------------------\n",
    "    if \"video\" in user_input.lower() and \"about\" in user_input.lower():\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in latest_video_chunks)\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{context}\\n\\nUser question: {user_input}\"})\n",
    "    else:\n",
    "        relevant_docs = vectorstore.similarity_search(user_input, k=3)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{context}\\n\\nUser question: {user_input}\"})\n",
    "\n",
    "    # ---------- GRAPH CALL -----------------------------------------\n",
    "    result = graph.invoke({\"messages\": messages})\n",
    "    assistant_reply = result[\"messages\"][-1].content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "    chat_history += [\n",
    "        {\"role\": \"user\", \"content\": user_input},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_reply},\n",
    "    ]\n",
    "    return \"\", chat_history\n",
    "\n",
    "def clear_conversation():\n",
    "    return \"\", []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06703c3-8551-43ac-963a-ba8895bf55e7",
   "metadata": {},
   "source": [
    "## 15. üñºÔ∏è Gradio User Interface for AI Tutor\n",
    "\n",
    "This section builds an interactive Gradio UI where users can:\n",
    "\n",
    "- üéõÔ∏è Select a tutor mode (Socratic, Quiz, GPU Guidance, etc.)\n",
    "- üí¨ Chat with the assistant in real-time\n",
    "- üìÑ Upload documents (`.pdf`, `.txt`) for RAG-based search\n",
    "- üé• Upload videos (`.mp4`) to extract and use transcripts\n",
    "- ‚è±Ô∏è Submit code for benchmarking and GPU conversion\n",
    "- üßπ Reset the conversation history\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Components:\n",
    "\n",
    "- **Radio Selector** (`tutor_select`) ‚Äî Sets system prompt for different tutoring behaviors.\n",
    "- **Chatbot Window** (`chatbot`) ‚Äî Shows back-and-forth interaction.\n",
    "- **Textbox + Button** (`query_input`, `submit_btn`) ‚Äî Where the user submits prompts.\n",
    "- **Document Upload** (`file_input`) ‚Äî Accepts `.pdf` or `.txt` files for semantic search.\n",
    "- **Video Upload** (`video_input`) ‚Äî Only appears if Video Tutor mode is selected.\n",
    "- **Clear Button** ‚Äî Resets the chat thread.\n",
    "\n",
    "All logic is tied to backend functions like:\n",
    "- `ask_graph()` for question handling\n",
    "- `process_document()` and `process_video()` for RAG ingestion\n",
    "- `set_bot()` to adjust tutor mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ba808ac-368d-4929-8d65-66f412c41fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "* Running on public URL: https://6325fb2ef5207ef8b1.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6325fb2ef5207ef8b1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# === Build Gradio UI ===\n",
    "with gr.Blocks(fill_height=True, fill_width=True) as demo:\n",
    "    gr.Markdown(\"## üß† Choose Your AI Tutor + üóÇÔ∏è Upload Supporting Documents\")\n",
    "\n",
    "    with gr.Row():\n",
    "        tutor_select = gr.Radio(\n",
    "            label=\"Choose your Tutor\", \n",
    "            choices=list(bot_prompts.keys()), \n",
    "            value=\"Python Tutor\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=350, type=\"messages\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            query_input = gr.Textbox(label=\"Enter text here\", placeholder=\"Ask something...\", lines=1)\n",
    "        with gr.Column(scale=1):\n",
    "            submit_btn = gr.Button(\"‚¨Ü\")\n",
    "            clear_btn = gr.Button(\"üßπ Clear Conversation\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"üìÑ Upload Document (.pdf or .txt )\", file_types=[\".pdf\", \".txt\"])\n",
    "        upload_status = gr.Textbox(label=\"Upload Status\", interactive=False)\n",
    "\n",
    "    with gr.Row(visible=False) as video_row:\n",
    "        video_input = gr.File(label=\"üé• Upload Video (.mp4)\", file_types=[\".mp4\"])\n",
    "        video_status = gr.Textbox(label=\"Video Status\", interactive=False)\n",
    "\n",
    "    # Bind buttons to functions\n",
    "    tutor_select.change(fn=set_bot, inputs=tutor_select, outputs=[query_input, chatbot,video_row])\n",
    "    submit_btn.click(fn=ask_graph, inputs=[query_input, chatbot], outputs=[query_input, chatbot])\n",
    "    query_input.submit(fn=ask_graph, inputs=[query_input, chatbot], outputs=[query_input, chatbot])\n",
    "    clear_btn.click(fn=clear_conversation, outputs=[query_input, chatbot])\n",
    "    file_input.change(fn=process_document, inputs=file_input, outputs=upload_status)\n",
    "    video_input.change(fn=process_video,  inputs=video_input, outputs=video_status)\n",
    "\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b506774-a284-47d9-a35d-e41590a9b2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai25.07",
   "language": "python",
   "name": "genai25.07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
